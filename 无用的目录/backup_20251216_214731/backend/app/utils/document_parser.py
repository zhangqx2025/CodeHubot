"""
æ–‡æ¡£è§£æå™¨
æ”¯æŒTXTå’ŒMarkdownæ ¼å¼çš„æ–‡æ¡£è§£æå’Œæ–‡æœ¬åˆ‡åˆ†
"""
from typing import List, Dict, Any, Optional, Tuple
import re
import chardet


class DocumentParser:
    """æ–‡æ¡£è§£æå™¨åŸºç±»"""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        Args:
            chunk_size: æ–‡æœ¬å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def parse(self, content: bytes) -> str:
        """
        è§£ææ–‡æ¡£å†…å®¹
        
        Args:
            content: æ–‡æ¡£å†…å®¹ï¼ˆbytesï¼‰
        
        Returns:
            str: è§£æåçš„æ–‡æœ¬
        """
        raise NotImplementedError
    
    def split_into_chunks(self, text: str, mode: str = 'fixed') -> List[Dict[str, Any]]:
        """
        å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºæ–‡æœ¬å—
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            mode: åˆ‡åˆ†æ¨¡å¼ ('fixed'|'paragraph'|'sentence'|'custom')
        
        Returns:
            List[Dict]: æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«contentã€indexã€char_countç­‰
        """
        if not text or not text.strip():
            return []
        
        if mode == 'paragraph':
            return self._split_by_paragraph(text)  # å•æ¢è¡Œ
        elif mode == 'paragraph_double':
            return self._split_by_paragraph_double(text)  # åŒæ¢è¡Œ
        elif mode == 'sentence':
            return self._split_by_sentence(text)
        else:  # fixed æˆ– custom
            return self._split_by_fixed_size(text)
    
    def _split_by_fixed_size(self, text: str) -> List[Dict[str, Any]]:
        """
        æŒ‰å›ºå®šå¤§å°åˆ‡åˆ†ï¼ˆç®€åŒ–å®‰å…¨ç‰ˆæœ¬ï¼‰
        
        æ ¸å¿ƒåŸåˆ™ï¼š
        1. æ°¸è¿œç¡®ä¿ start å‘å‰ç§»åŠ¨
        2. ä¸¥æ ¼é™åˆ¶è¿­ä»£æ¬¡æ•°
        3. é¿å…å¤æ‚çš„è¾¹ç•Œåˆ¤æ–­
        """
        import logging
        logger = logging.getLogger(__name__)
        
        chunks = []
        chunk_index = 0
        start = 0
        text_length = len(text)
        
        # ğŸš¨ ä¸¥æ ¼çš„è¿­ä»£é™åˆ¶ï¼šæœ€å¤šæ–‡æœ¬é•¿åº¦çš„2å€ï¼Œç»å¯¹ä¸è¶…è¿‡1000æ¬¡
        max_iterations = min((text_length // max(1, self.chunk_size)) * 2 + 10, 1000)
        iteration = 0
        
        logger.info(f"å¼€å§‹æŒ‰å›ºå®šå¤§å°åˆ‡åˆ†ï¼Œæ–‡æœ¬é•¿åº¦: {text_length}, chunk_size: {self.chunk_size}, overlap: {self.chunk_overlap}, max_iter: {max_iterations}")
        
        while start < text_length and iteration < max_iterations:
            iteration += 1
            
            # ğŸš¨ æ¯10æ¬¡è¿­ä»£è¾“å‡ºè¿›åº¦
            if iteration % 10 == 0:
                logger.info(f"åˆ‡åˆ†è¿›åº¦: è¿­ä»£ {iteration}/{max_iterations}, start={start}/{text_length} ({start*100//text_length}%)")
            
            # ç¡®å®šå½“å‰å—çš„ç»“æŸä½ç½®ï¼ˆç®€å•ç›´æ¥ï¼‰
            end = min(start + self.chunk_size, text_length)
            
            # ğŸš¨ ä¸¥æ ¼æ£€æŸ¥ï¼šend å¿…é¡»å¤§äº start
            if end <= start:
                logger.error(f"æ£€æµ‹åˆ°å¼‚å¸¸ï¼šend({end}) <= start({start})ï¼Œå¼ºåˆ¶ç»ˆæ­¢ï¼")
                break
            
            # æå–æ–‡æœ¬å—ï¼ˆä¸ä½¿ç”¨stripï¼Œä¿æŒåŸå§‹é•¿åº¦ï¼‰
            chunk_text = text[start:end]
            
            # åªè¿‡æ»¤å®Œå…¨ç©ºç™½çš„å—
            if chunk_text and chunk_text.strip():
                chunks.append({
                    'content': chunk_text.strip(),
                    'chunk_index': chunk_index,
                    'char_count': len(chunk_text.strip()),
                    'token_count': self.estimate_token_count(chunk_text),
                    'metadata': {
                        'start_position': start,
                        'end_position': end,
                        'split_mode': 'fixed'
                    }
                })
                chunk_index += 1
            
            # ğŸš¨ å®‰å…¨çš„ä½ç½®ç§»åŠ¨ï¼šç¡®ä¿è‡³å°‘å‰è¿› 1 ä¸ªå­—ç¬¦
            # ä¼˜å…ˆä½¿ç”¨ overlapï¼Œä½†å¦‚æœ overlap å¤ªå¤§ï¼Œè‡³å°‘å‰è¿› chunk_size çš„ 1/4
            min_step = max(1, self.chunk_size // 4)
            next_start = end - self.chunk_overlap
            
            if next_start <= start:
                # å¼ºåˆ¶è‡³å°‘å‰è¿› min_step
                next_start = start + min_step
                logger.warning(f"æ£€æµ‹åˆ°ä½ç½®æœªå‰è¿›ï¼Œå¼ºåˆ¶å‰è¿› {min_step} ä¸ªå­—ç¬¦ (start={start} -> {next_start})")
            
            # ğŸš¨ æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœ next_start ä»ç„¶æ²¡æœ‰å‰è¿›ï¼Œç›´æ¥è·³åˆ° end
            if next_start <= start:
                next_start = end
                logger.error(f"ä¸¥é‡é”™è¯¯ï¼šä½ç½®ä»æœªå‰è¿›ï¼Œå¼ºåˆ¶è·³åˆ° end={end}")
            
            start = next_start
        
        # ğŸš¨ æ£€æŸ¥æ˜¯å¦å› ä¸ºè¿­ä»£é™åˆ¶è€Œç»ˆæ­¢
        if iteration >= max_iterations and start < text_length:
            logger.error(f"åˆ‡åˆ†å› è¿­ä»£æ¬¡æ•°é™åˆ¶è€Œç»ˆæ­¢ï¼å·²è¿­ä»£ {iteration} æ¬¡ï¼Œstart={start}, text_length={text_length}")
            logger.error(f"å‰©ä½™æ–‡æœ¬é•¿åº¦: {text_length - start} å­—ç¬¦ï¼Œå·²ç”Ÿæˆ {len(chunks)} ä¸ªå—")
        else:
            logger.info(f"åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œè¿­ä»£ {iteration} æ¬¡")
        
        return chunks
    
    def _split_by_paragraph(self, text: str) -> List[Dict[str, Any]]:
        """æŒ‰æ®µè½åˆ‡åˆ†ï¼ˆä½¿ç”¨å•æ¢è¡Œç¬¦ï¼‰- æ¯ä¸ªæ¢è¡Œåˆ†éš”çš„å†…å®¹å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å—"""
        # æŒ‰å•æ¢è¡Œç¬¦åˆ†å‰²ï¼Œæ¯ä¸ªæ¢è¡Œåˆ†éš”çš„å†…å®¹å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å—
        lines = text.split('\n')
        chunks = []
        start_pos = 0
        
        for chunk_index, line in enumerate(lines):
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                start_pos += 1  # æ¢è¡Œç¬¦å 1ä¸ªå­—ç¬¦
                continue
            
            # æ¯ä¸ªéç©ºè¡Œå°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ–‡æœ¬å—
            chunks.append({
                'content': line,
                'chunk_index': chunk_index,
                'char_count': len(line),
                'token_count': self.estimate_token_count(line),
                'metadata': {
                    'start_position': start_pos,
                    'end_position': start_pos + len(line),
                    'split_mode': 'paragraph'
                }
            })
            start_pos += len(line) + 1  # å†…å®¹é•¿åº¦ + æ¢è¡Œç¬¦
        
        return chunks
    
    def _split_by_paragraph_double(self, text: str) -> List[Dict[str, Any]]:
        """æŒ‰æ®µè½åˆ‡åˆ†ï¼ˆä½¿ç”¨åŒæ¢è¡Œç¬¦ï¼‰
        
        æ¯ä¸ªåŒæ¢è¡Œç¬¦åˆ†éš”çš„æ®µè½ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„chunkï¼Œä¸åˆå¹¶æ®µè½
        å¦‚æœæ®µè½æœ¬èº«è¶…è¿‡chunk_sizeï¼Œåˆ™æŒ‰å›ºå®šå¤§å°åˆ‡åˆ†
        """
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        paragraphs = re.split(r'\n\n+', text)
        chunks = []
        chunk_index = 0
        start_pos = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                # è·³è¿‡ç©ºæ®µè½ï¼Œä½†éœ€è¦æ›´æ–°ä½ç½®ï¼ˆåŒæ¢è¡Œç¬¦å 2ä¸ªå­—ç¬¦ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œæ— æ³•ç²¾ç¡®è®¡ç®—ä½ç½®ï¼Œå› ä¸ºä¸çŸ¥é“å‰é¢æœ‰å¤šå°‘ä¸ªæ¢è¡Œ
                continue
            
            # æ¯ä¸ªæ®µè½ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„chunkï¼Œä¸åˆå¹¶
            # å¦‚æœæ®µè½æœ¬èº«å¤ªé•¿ï¼ŒæŒ‰å›ºå®šå¤§å°åˆ‡åˆ†
            if len(para) > self.chunk_size:
                # æ®µè½å¤ªé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
                sub_chunks = self._split_long_paragraph(para, chunk_index, start_pos)
                chunks.extend(sub_chunks)
                chunk_index += len(sub_chunks)
                # æ›´æ–°ä½ç½®ï¼ˆè¿‘ä¼¼å€¼ï¼Œå› ä¸ºæ— æ³•ç²¾ç¡®è®¡ç®—åŒæ¢è¡Œç¬¦çš„ä½ç½®ï¼‰
                start_pos += len(para) + 2
            else:
                # æ®µè½å¤§å°åˆé€‚ï¼Œä½œä¸ºä¸€ä¸ªchunk
                chunks.append({
                    'content': para,
                    'chunk_index': chunk_index,
                    'char_count': len(para),
                    'token_count': self.estimate_token_count(para),
                    'metadata': {
                        'start_position': start_pos,
                        'end_position': start_pos + len(para),
                        'split_mode': 'paragraph_double'
                    }
                })
                chunk_index += 1
                # æ›´æ–°ä½ç½®ï¼ˆè¿‘ä¼¼å€¼ï¼‰
                start_pos += len(para) + 2
        
        return chunks
    
    def _split_by_sentence(self, text: str) -> List[Dict[str, Any]]:
        """æŒ‰å¥å­åˆ‡åˆ†"""
        # æŒ‰å¥å·ã€é—®å·ã€å¹å·åˆ†å‰²å¥å­
        sentences = re.split(r'([ã€‚ï¼Ÿï¼ï¼›])', text)
        
        # é‡æ–°ç»„åˆå¥å­å’Œæ ‡ç‚¹
        combined_sentences = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                combined_sentences.append(sentences[i] + sentences[i + 1])
            else:
                combined_sentences.append(sentences[i])
        
        chunks = []
        chunk_index = 0
        current_chunk = ""
        start_pos = 0
        
        for sentence in combined_sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°å¥å­ä¸è¶…è¿‡å—å¤§å°ï¼Œåˆå¹¶
            if len(current_chunk) + len(sentence) < self.chunk_size:
                current_chunk += sentence
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append({
                        'content': current_chunk,
                        'chunk_index': chunk_index,
                        'char_count': len(current_chunk),
                        'token_count': self.estimate_token_count(current_chunk),
                        'metadata': {
                            'start_position': start_pos,
                            'end_position': start_pos + len(current_chunk),
                            'split_mode': 'sentence'
                        }
                    })
                    chunk_index += 1
                    start_pos += len(current_chunk)
                
                current_chunk = sentence
        
        # ä¿å­˜æœ€åä¸€å—
        if current_chunk:
            chunks.append({
                'content': current_chunk,
                'chunk_index': chunk_index,
                'char_count': len(current_chunk),
                'token_count': self.estimate_token_count(current_chunk),
                'metadata': {
                    'start_position': start_pos,
                    'end_position': start_pos + len(current_chunk),
                    'split_mode': 'sentence'
                }
            })
        
        return chunks
    
    def _split_long_paragraph(self, paragraph: str, start_index: int, start_pos: int) -> List[Dict[str, Any]]:
        """
        åˆ‡åˆ†è¿‡é•¿çš„æ®µè½ï¼ˆç®€åŒ–å®‰å…¨ç‰ˆæœ¬ï¼‰
        
        ğŸš¨ å…³é”®ä¿®å¤ï¼šç¡®ä¿ start å§‹ç»ˆå‘å‰ç§»åŠ¨ï¼Œå³ä½¿ chunk_text ä¸ºç©º
        """
        import logging
        logger = logging.getLogger(__name__)
        
        chunks = []
        start = 0
        para_len = len(paragraph)
        max_iterations = (para_len // max(1, self.chunk_size)) + 10
        iteration = 0
        
        while start < para_len and iteration < max_iterations:
            iteration += 1
            end = min(start + self.chunk_size, para_len)
            
            # ğŸš¨ æ£€æŸ¥ï¼šend å¿…é¡»å¤§äº start
            if end <= start:
                logger.error(f"_split_long_paragraph: end({end}) <= start({start})ï¼Œç»ˆæ­¢å¾ªç¯")
                break
            
            chunk_text = paragraph[start:end].strip()
            
            if chunk_text:
                chunks.append({
                    'content': chunk_text,
                    'chunk_index': start_index + len(chunks),
                    'char_count': len(chunk_text),
                    'token_count': self.estimate_token_count(chunk_text),
                    'metadata': {
                        'start_position': start_pos + start,
                        'end_position': start_pos + end,
                        'split_mode': 'paragraph_long'
                    }
                })
            
            # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ— è®º chunk_text æ˜¯å¦ä¸ºç©ºï¼Œéƒ½è¦ç§»åŠ¨ start
                start = end
        
        if iteration >= max_iterations:
            logger.error(f"_split_long_paragraph: è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}")
        
        return chunks
    
    @staticmethod
    def estimate_token_count(text: str) -> int:
        """
        ä¼°ç®—tokenæ•°é‡
        ä¸­æ–‡æŒ‰1ä¸ªå­—ç¬¦=1ä¸ªtokenï¼Œè‹±æ–‡æŒ‰4ä¸ªå­—ç¬¦=1ä¸ªtokenä¼°ç®—
        ä¼˜åŒ–ç‰ˆï¼šé¿å…ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œç›´æ¥éå†å­—ç¬¦
        """
        if not text:
            return 0
        
        chinese_chars = 0
        # âœ… ç›´æ¥éå†ï¼Œæ¯”æ­£åˆ™å¿«å¾ˆå¤š
        for char in text:
            # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ—¥éŸ©ç»Ÿä¸€è¡¨æ„æ–‡å­—ï¼‰
            if '\u4e00' <= char <= '\u9fff':
                chinese_chars += 1
        
        other_chars = len(text) - chinese_chars
        return chinese_chars + (other_chars // 4)
    
    @staticmethod
    def detect_encoding(content: bytes, return_confidence: bool = False) -> str:
        """
        æ£€æµ‹æ–‡ä»¶ç¼–ç 
        
        Args:
            content: æ–‡ä»¶å†…å®¹ï¼ˆbytesï¼‰
            return_confidence: æ˜¯å¦è¿”å›ç½®ä¿¡åº¦ä¿¡æ¯
        
        Returns:
            str: æ£€æµ‹åˆ°çš„ç¼–ç åç§°ï¼ˆå¦‚'utf-8', 'gbk', 'gb2312'ç­‰ï¼‰
            æˆ– tuple: (ç¼–ç åç§°, ç½®ä¿¡åº¦) if return_confidence=True
        """
        # ä½¿ç”¨ chardet æ£€æµ‹ç¼–ç 
        detected = chardet.detect(content)
        encoding = detected.get('encoding', 'utf-8')
        confidence = detected.get('confidence', 0.0)
        
        # å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œå°è¯•å¸¸è§ç¼–ç 
        if confidence < 0.7:
            # ä¸­æ–‡å¸¸è§ç¼–ç åˆ—è¡¨
            common_encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'big5']
            
            for enc in common_encodings:
                try:
                    content.decode(enc)
                    encoding = enc
                    confidence = 0.99  # æ‰‹åŠ¨è®¾ç½®é«˜ç½®ä¿¡åº¦
                    break
                except (UnicodeDecodeError, LookupError):
                    continue
        
        # ç¼–ç åç§°æ ‡å‡†åŒ–
        if encoding:
            encoding = encoding.lower()
            # GB2312 æ˜¯ GBK çš„å­é›†ï¼Œä½¿ç”¨ GBK æ›´ä¿é™©
            if encoding in ['gb2312', 'gb18030']:
                encoding = 'gbk'
        else:
            encoding = 'utf-8'
        
        if return_confidence:
            return encoding, confidence
        return encoding


class TxtParser(DocumentParser):
    """çº¯æ–‡æœ¬æ–‡æ¡£è§£æå™¨"""
    
    def parse(self, content: bytes) -> str:
        """
        è§£æTXTæ–‡ä»¶ï¼Œæ™ºèƒ½å¤„ç†ç¼–ç 
        
        Args:
            content: æ–‡ä»¶å†…å®¹ï¼ˆbytesï¼‰
        
        Returns:
            str: è§£æåçš„æ–‡æœ¬
        
        Raises:
            ValueError: å¦‚æœæ–‡ä»¶æ— æ³•è§£ç 
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        encoding, confidence = self.detect_encoding(content, return_confidence=True)
        logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding} (ç½®ä¿¡åº¦: {confidence:.2%})")
        
        # å°è¯•ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç 
        text = None
        try:
            text = content.decode(encoding)
            logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è§£ææ–‡ä»¶")
        except (UnicodeDecodeError, LookupError) as e:
            logger.warning(f"ä½¿ç”¨ {encoding} è§£ç å¤±è´¥: {str(e)}")
            
            # å°è¯•å¸¸è§ç¼–ç 
            fallback_encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'latin-1', 'cp1252']
            for fallback_enc in fallback_encodings:
                if fallback_enc == encoding:
                    continue  # è·³è¿‡å·²ç»å°è¯•è¿‡çš„
                try:
                    text = content.decode(fallback_enc)
                    logger.info(f"å›é€€ä½¿ç”¨ {fallback_enc} ç¼–ç æˆåŠŸ")
                    break
                except (UnicodeDecodeError, LookupError):
                    continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨ UTF-8 å¹¶å¿½ç•¥é”™è¯¯
            if text is None:
                logger.warning("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥ï¼Œä½¿ç”¨ UTF-8 å¹¶å¿½ç•¥é”™è¯¯å­—ç¬¦")
                text = content.decode('utf-8', errors='replace')
                # æ›¿æ¢æ‰ä¹±ç å­—ç¬¦
                text = text.replace('ï¿½', '')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤šçš„ä¹±ç å­—ç¬¦ï¼ˆå¯èƒ½ç¼–ç é”™è¯¯ï¼‰
        if text:
            invalid_ratio = text.count('ï¿½') / len(text) if len(text) > 0 else 0
            if invalid_ratio > 0.1:  # è¶…è¿‡10%æ˜¯ä¹±ç 
                logger.error(f"æ–‡ä»¶åŒ…å«è¿‡å¤šä¹±ç å­—ç¬¦ ({invalid_ratio:.1%})ï¼Œå¯èƒ½ç¼–ç è¯†åˆ«é”™è¯¯")
                raise ValueError(f"æ–‡ä»¶ç¼–ç é”™è¯¯ï¼ŒåŒ…å« {invalid_ratio:.1%} çš„ä¹±ç å­—ç¬¦ã€‚è¯·ç¡®ä¿æ–‡ä»¶ä½¿ç”¨ UTF-8ã€GBK æˆ– GB2312 ç¼–ç ")
        
        # è§„èŒƒåŒ–æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        # å»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™ä¸€ä¸ªï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # å»é™¤ BOM æ ‡è®°
        if text.startswith('\ufeff'):
            text = text[1:]
        
        return text.strip()


class MarkdownParser(DocumentParser):
    """Markdownæ–‡æ¡£è§£æå™¨"""
    
    def parse(self, content: bytes) -> str:
        """
        è§£æMarkdownæ–‡ä»¶ï¼Œæ™ºèƒ½å¤„ç†ç¼–ç 
        
        Args:
            content: æ–‡ä»¶å†…å®¹ï¼ˆbytesï¼‰
        
        Returns:
            str: è§£æåçš„æ–‡æœ¬
        
        Raises:
            ValueError: å¦‚æœæ–‡ä»¶æ— æ³•è§£ç 
        """
        import logging
        logger = logging.getLogger(__name__)
        
        text = None
        
        # Markdowné€šå¸¸æ˜¯UTF-8ç¼–ç ï¼Œä¼˜å…ˆå°è¯•
        try:
            text = content.decode('utf-8')
            logger.info("æˆåŠŸä½¿ç”¨ UTF-8 ç¼–ç è§£æ Markdown æ–‡ä»¶")
        except UnicodeDecodeError:
            logger.warning("UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç ")
            
            # å›é€€åˆ°è‡ªåŠ¨æ£€æµ‹
            encoding, confidence = self.detect_encoding(content, return_confidence=True)
            logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding} (ç½®ä¿¡åº¦: {confidence:.2%})")
            
            try:
                text = content.decode(encoding)
                logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è§£ææ–‡ä»¶")
            except (UnicodeDecodeError, LookupError):
                logger.warning(f"ä½¿ç”¨ {encoding} è§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ")
                
                # å°è¯•å…¶ä»–å¸¸è§ç¼–ç 
                fallback_encodings = ['gbk', 'gb2312', 'gb18030', 'latin-1']
                for fallback_enc in fallback_encodings:
                    if fallback_enc == encoding:
                        continue
                    try:
                        text = content.decode(fallback_enc)
                        logger.info(f"å›é€€ä½¿ç”¨ {fallback_enc} ç¼–ç æˆåŠŸ")
                        break
                    except (UnicodeDecodeError, LookupError):
                        continue
                
                # æœ€åçš„å›é€€ï¼šUTF-8 with replace
                if text is None:
                    logger.warning("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥ï¼Œä½¿ç”¨ UTF-8 å¹¶æ›¿æ¢é”™è¯¯å­—ç¬¦")
                    text = content.decode('utf-8', errors='replace')
                    text = text.replace('ï¿½', '')
        
        # æ£€æŸ¥ä¹±ç æ¯”ä¾‹
        if text:
            invalid_ratio = text.count('ï¿½') / len(text) if len(text) > 0 else 0
            if invalid_ratio > 0.1:
                logger.error(f"Markdown æ–‡ä»¶åŒ…å«è¿‡å¤šä¹±ç å­—ç¬¦ ({invalid_ratio:.1%})")
                raise ValueError(f"æ–‡ä»¶ç¼–ç é”™è¯¯ï¼ŒåŒ…å« {invalid_ratio:.1%} çš„ä¹±ç å­—ç¬¦ã€‚å»ºè®®ä½¿ç”¨ UTF-8 ç¼–ç ä¿å­˜ Markdown æ–‡ä»¶")
        
        # è§„èŒƒåŒ–æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        # å»é™¤ BOM æ ‡è®°
        if text.startswith('\ufeff'):
            text = text[1:]
        
        return text.strip()
    
    def split_into_chunks(self, text: str) -> List[Dict[str, Any]]:
        """
        Markdownæ™ºèƒ½åˆ‡åˆ†
        ä¼˜å…ˆæŒ‰æ ‡é¢˜å±‚çº§åˆ‡åˆ†ï¼Œå…¶æ¬¡æŒ‰æ®µè½
        """
        if not text or not text.strip():
            return []
        
        # å°è¯•æŒ‰æ ‡é¢˜åˆ‡åˆ†
        chunks = self._split_by_headers(text)
        
        # å¦‚æœæ²¡æœ‰æ ‡é¢˜æˆ–è€…å—å¤ªå¤§ï¼Œä½¿ç”¨é»˜è®¤åˆ‡åˆ†
        if not chunks or any(len(c['content']) > self.chunk_size * 2 for c in chunks):
            return super().split_into_chunks(text)
        
        return chunks
    
    def _split_by_headers(self, text: str) -> List[Dict[str, Any]]:
        """æŒ‰Markdownæ ‡é¢˜åˆ‡åˆ†"""
        # åŒ¹é…Markdownæ ‡é¢˜ï¼ˆ# ## ### ç­‰ï¼‰
        header_pattern = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)
        
        chunks = []
        chunk_index = 0
        last_pos = 0
        
        # æå–æ‰€æœ‰æ ‡é¢˜ä½ç½®
        headers = [(m.start(), m.end(), m.group(1), m.group(2)) 
                   for m in header_pattern.finditer(text)]
        
        if not headers:
            return []
        
        # æŒ‰æ ‡é¢˜åˆ‡åˆ†
        for i, (start, end, level, title) in enumerate(headers):
            # è·å–åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ä¹‹å‰çš„å†…å®¹
            if i < len(headers) - 1:
                next_start = headers[i + 1][0]
                chunk_text = text[start:next_start].strip()
            else:
                chunk_text = text[start:].strip()
            
            if chunk_text:
                chunks.append({
                    'content': chunk_text,
                    'chunk_index': chunk_index,
                    'char_count': len(chunk_text),
                    'token_count': self.estimate_token_count(chunk_text),
                    'metadata': {
                        'header_level': len(level),
                        'header_title': title,
                        'start_position': start,
                        'end_position': start + len(chunk_text)
                    }
                })
                chunk_index += 1
        
        return chunks


def get_parser(file_type: str, chunk_size: int = 500, chunk_overlap: int = 50) -> DocumentParser:
    """
    è·å–æ–‡æ¡£è§£æå™¨
    
    Args:
        file_type: æ–‡ä»¶ç±»å‹ï¼ˆtxt/mdï¼‰
        chunk_size: æ–‡æœ¬å—å¤§å°
        chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°
    
    Returns:
        DocumentParser: å¯¹åº”çš„è§£æå™¨å®ä¾‹
    """
    if file_type.lower() == 'txt':
        return TxtParser(chunk_size, chunk_overlap)
    elif file_type.lower() == 'md':
        return MarkdownParser(chunk_size, chunk_overlap)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")


def parse_and_split_document(
    content: bytes, 
    file_type: str, 
    chunk_size: int = 500, 
    chunk_overlap: int = 50,
    split_mode: str = 'fixed'
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    è§£ææ–‡æ¡£å¹¶åˆ‡åˆ†ä¸ºæ–‡æœ¬å—ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        content: æ–‡æ¡£å†…å®¹ï¼ˆbytesï¼‰
        file_type: æ–‡ä»¶ç±»å‹ï¼ˆtxt/mdï¼‰
        chunk_size: æ–‡æœ¬å—å¤§å°
        chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°
        split_mode: åˆ‡åˆ†æ¨¡å¼ ('fixed'|'paragraph'|'sentence'|'custom')
    
    Returns:
        tuple: (è§£æåçš„æ–‡æœ¬, æ–‡æœ¬å—åˆ—è¡¨)
    """
    parser = get_parser(file_type, chunk_size, chunk_overlap)
    text = parser.parse(content)
    chunks = parser.split_into_chunks(text, mode=split_mode)
    return text, chunks

